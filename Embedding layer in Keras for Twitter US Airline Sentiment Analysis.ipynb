{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding layer in Keras for Twitter US Airline Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The embedding layer in keras can learn the word embeddings for our corpus which can then used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use(\"dark_background\")\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>airline</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>dhepburn said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>plus added commercials experience tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>not today must mean need take another trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>1</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>1</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>really big bad thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  airline_sentiment negativereason         airline  \\\n",
       "0  570306133677760513                  0            NaN  Virgin America   \n",
       "1  570301130888122368                  0            NaN  Virgin America   \n",
       "2  570301083672813571                  0            NaN  Virgin America   \n",
       "3  570301031407624196                  1     Bad Flight  Virgin America   \n",
       "4  570300817074462722                  1     Can't Tell  Virgin America   \n",
       "\n",
       "                                          text_clean  \n",
       "0                                      dhepburn said  \n",
       "1            plus added commercials experience tacky  \n",
       "2         not today must mean need take another trip  \n",
       "3  really aggressive blast obnoxious entertainmen...  \n",
       "4                               really big bad thing  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twDF = pd.read_csv(\"Dataset/tweets_preprocessed.csv\").drop(['Unnamed: 0'],axis=1)\n",
    "twDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = twDF['text_clean'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dhepburn said',\n",
       " 'plus added commercials experience tacky',\n",
       " 'not today must mean need take another trip',\n",
       " 'really aggressive blast obnoxious entertainment guests faces amp little recourse',\n",
       " 'really big bad thing']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of sentences for further processsing\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing less frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/57179045/how-to-remove-less-frequent-words-from-pandas-dataframe/57179318\n",
    "all_ = [x for y in corpus for x in y.split(' ') ]\n",
    "a,b = np.unique(all_, return_counts = True)\n",
    "to_remove = a[b<3]\n",
    "corpus = [' '.join(np.array(y.split(' '))[~np.isin(y.split(' '), to_remove)]) for y in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = twDF['airline_sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words in corpus are 4442\n"
     ]
    }
   ],
   "source": [
    "all_ = [x for y in docs for x in y.split(' ') ]\n",
    "a,b = np.unique(all_, return_counts = True)\n",
    "print(\"number of unique words in corpus are\",len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = len(max(docs, key = len).split(\" \"))\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "1.max_length is the length of longest sentence in our corpus after removing less frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding our corpus\n",
    "input_len = len(a)*5\n",
    "corp_enc = [one_hot(d, input_len) for d in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11196], [16612, 8874, 5045, 12015], [15993, 10173, 8192, 9244, 8845, 3938, 408, 7877], [17565, 15040, 14892, 3651, 7618, 6124, 14442, 19087], [17565, 2613, 8994, 9463], [14119, 21073, 3899, 13830, 15976, 15642, 15993, 20029, 17565, 8994, 9463, 2973, 14056], [17525, 282, 13549, 145, 19267, 8762, 2678, 12453], [17565, 12780, 9217, 10181, 17759, 4860, 7196, 4494, 17253], [12163, 20340, 15993], [3297, 15516, 8195, 9776, 3312]]\n"
     ]
    }
   ],
   "source": [
    "print(corp_enc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11196     0     0 ...     0     0     0]\n",
      " [16612  8874  5045 ...     0     0     0]\n",
      " [15993 10173  8192 ...     0     0     0]\n",
      " ...\n",
      " [19736 18698 20245 ...     0     0     0]\n",
      " [14412   798 15976 ...     0     0     0]\n",
      " [16042 16539  8845 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "corp_pad = pad_sequences(corp_enc, maxlen=max_length, padding='post')\n",
    "print(corp_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "1.one_hot converts each word into integer encoding (sequences)\n",
    "\n",
    "2.pad_sequences pads the sequences with zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with keras embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(corp_pad, target, test_size=0.33, random_state=42, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 20, 200)           4442000   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 4001      \n",
      "=================================================================\n",
      "Total params: 4,446,001\n",
      "Trainable params: 4,446,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "import keras\n",
    "from keras.layers import Dropout\n",
    "vector_size = 200\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_len, vector_size, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[keras.metrics.Recall()])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fitting model\n",
    "result=model.fit(X_train, y_train,validation_split=0.33, epochs=6,verbose=0,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "loss, recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('recall:',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(result.history[\"loss\"], label=\"training loss\")\n",
    "plt.plot(result.history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A recall of 0.859 is obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
